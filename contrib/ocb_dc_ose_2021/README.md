# Pipeline: Inference on real datasets

## Introduction

This pipeline is used to compute metrics according to the framework of the
[SSH Mapping Data Challenge 2021a](https://github.com/ocean-data-challenges/2021a_SSH_mapping_OSE).

It performs the following steps:

1. Download the partially pre-processed data separately:
    - Observation files: alg, h2ag, j2g, j2n, j3 and s3a
    - Reference file: c2
2. Concatenate the observation files into a single observation file
3. Grid the concatenated observation file and the reference file
4. Use 4DVarNet to predict (requires a checkpoint and concatenated observation
    file, generates and stores patches' files)
5. Reconstruct the mapping from the various patches generated by 4DVarNet
6. Restrict the mapping to c2's along track
7. Compute metrics:
    - Spectral score: $\lambda_x$
    - Normalised RMSE: $\mu$
8. Show the leaderboard

Basic knowledges in [DVC](https://dvc.org/) would be helpful but not mandatory
to run the pipeline.


## Install

We recommand using conda/mamba.

1. Create a directory and a Python environment:
    ```sh
    mkdir ocb-starter
    cd ocb-starter
    mamba create -n ocb-starter  # either `mamba` or `conda`
    mamba activate ocb-starter
    ```
    In the following, we will assume you are executing the next commands from
    this directory (`ocb_starter`).
2. (optional¹)
    Clone [4dvarnet-starter](https://github.com/CIA-Oceanix/4dvarnet-starter)
    and update your environment with its dependencies:
    ```sh
    git clone git@github.com:CIA-Oceanix/4dvarnet-starter.git
    mamba env update -f 4dvarnet-starter/environment.yaml
    ```
3. Clone [my-ocb](https://github.com/quentinf00/my_ocb) and update your
    environment with its dependencies:
    ```sh
    git clone git@github.com:quentinf00/my_ocb.git
    mamba env update -f my_ocb/env.yaml
    ```
4. Update your environment with internal dependencies:
    ```sh
    # install datachallenge modules
    pip install -q -e my_ocb/modules/qf_interp_grid_on_track
    pip install -q -e my_ocb/modules/alongtrack_lambdax
    pip install -q -e my_ocb/modules/dz_alongtrack_mu
    pip install -q -e my_ocb/modules/qf_hydra_recipes
    pip install -q -e my_ocb/modules/qf_pipeline

    # install 4dvarnet-starter modules
    pip install -q -e 4dvarnet-starter/contrib/qf_merge_patches
    pip install -q -e 4dvarnet-starter/contrib/dz_lit_patch_predict
    pip install -q -e 4dvarnet-starter/contrib/qf_predict_ose_ssh_dc_2021
    ```
5. Move to the pipeline directory and initialise DVC:
    ```sh
    cd 4dvarnet-starter/contrib/ocb_dc_ose_2021/
    dvc init
    ```

¹ Only if you have already cloned 4dvarnet-starter and do not mind altering
its environment.


## Run an inference

Create a file at `4dvarnet-starter/contrib/ocb_dc_ose_2021/params.yaml`
containing these informations:

```yaml
method: <method_identifier>  # e. g. '4dvarnet-gf-5nad'
resolution: <resolution>     # e. g. 0.125 for 1/8 or 0.05 for 1/20
cfg_path: <cfg_path>
ckpt_path: <ckpt_path>
```

Where:
- **method_identifier** is a name of your choice to identify the method you
    are evaluating in inference;
- **cfg_path** is the path to the configuration generated by hydra (stored by
    default at `4dvarnet-starter/outputs/<date>/<time>/.hydra/config.yaml`);
- **ckpt_path** is the path to the checkpoint generated by PyTorch Lightning
    (stored by default at
    `4dvarnet-starter/outputs/<date>/<time>/<xp>/checkpoints/<ckpt_name>.ckpt`).

Then, to run an inference on real data, do:

```sh
cd 4dvarnet-starter/contrib/ocb_dc_ose_2021
dvc repro
```

If you want to modify the pipelines, check the files located at
`4dvarnet-starter/contrib/ocb_dc_ose_2021/dvc.yaml`.

See [DVC official website](https://dvc.org/) for more information about
pipelines.